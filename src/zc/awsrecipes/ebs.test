Utility scripts
===============

lebs
----

Deploy or update a logical volume defined at a given ZooKeeper path.

Usage: ``lebs ZOO PATH``

where:

ZOO
  A ZooKeeper connection string.

PATH
  A path in ZooKeeper where a logical ebs volume is defined.  It must
  have a type property with the value: ``lebs``

Notes:

- The ZooKeeper tree must have a ``/hosts`` node containing a ``cluster``
  property naming the VPC/cluster that will be used.

Let's set up a sample tree::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 2
           replicas = 1, 2

  /hosts
    cluster = 'test_cluster'

.. -> tree

    >>> import zc.zk
    >>> zk = zc.zk.ZK('zookeeper.example.com:2181')
    >>> zk.import_tree(tree)

    >>> import pkg_resources
    >>> dist = pkg_resources.working_set.find(
    ...     pkg_resources.Requirement.parse('zc.awsrecipes'))
    >>> lebs = pkg_resources.load_entry_point(dist, 'console_scripts', 'lebs')
    >>> lebs(['zookeeper.example.com:2181', '/example/example.com/volume'])
    created /example/example.com/volume 1-1
    created /example/example.com/volume 1-2
    created /example/example.com/volume 2-1
    created /example/example.com/volume 2-2

    >>> import boto.ec2
    >>> conn = boto.ec2.connect_to_region('test_region')
    >>> for volume in conn.volumes:
    ...     print volume
    Volume({'id': 'vol0',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 1-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '1'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol1',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 1-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '1'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol2',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol3',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})

If we change the size after there are existing volumes, we'll get an error::

  /example
    /example.com
       /volume : lebs
           size = 2
           n = 2
           replicas = 1, 2

.. -> tree

    >>> zk.import_tree(tree)
    >>> lebs(['zookeeper.example.com:2181', '/example/example.com/volume'])
    Traceback (most recent call last):
    ...
    AssertionError: Existing volumne, /example/example.com/volume 1-1, has size 1

If we increase the number of volumes, or add a replica, additional
volumes are added as necessary::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 4
           replicas = 3, 2

.. -> tree

    >>> zk.import_tree(tree)
    >>> lebs(['zookeeper.example.com:2181', '/example/example.com/volume'])
    ... # doctest: +NORMALIZE_WHITESPACE
    created /example/example.com/volume 3-1
    created /example/example.com/volume 3-2
    created /example/example.com/volume 3-3
    created /example/example.com/volume 3-4
    exists /example/example.com/volume 2-1
    exists /example/example.com/volume 2-2
    created /example/example.com/volume 2-3
    created /example/example.com/volume 2-4
    Unused: ['/example/example.com/volume 1-1',
             '/example/example.com/volume 1-2']

Note that we stopped using replica 1.  The corresponding unused
volumes were noted, but not removed.  We're too cowardly to remove
disk volumes.

Once the volumes have been created, we're ready to set up storage
servers.  We'll start with one::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 4
           replicas = 3, 2

       /storage.example.com : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/volume 2'

.. -> tree

    >>> zk.import_tree(tree)

To add a server, we'll use the storage-server command:

    >>> server = pkg_resources.load_entry_point(
    ...     dist, 'console_scripts', 'storage-server')


    >>> server(['zookeeper.example.com:2181',
    ...        '/example/example.com/storage.example.com'])


At this point, we have a storage server:

    >>> for instance in conn.instances:
    ...     print instance
    Instance({'attrs': {'groupSet': [Resource({'id': 'gr1',
     'name': 'default',
     'tags': {},
     'vpc_id': 'vpc1'})],
               'userData': "#!/bin/sh\necho 'example,storage' > /etc/zim/role\nhostname storage.example.com\n/usr/bin/yum -y install awsrecipes\n/opt/awsrecipes/bin/setup_volumes /example/example.com/storage.example.com\n"},
     'id': 'inst0',
     'tags': {'Name': 'storage.example.com',
              'creator': 'testy (Testy Tester)'}})

And the relevent volumes are attached:

    >>> for volume in conn.volumes:
    ...     print volume
    Volume({'id': 'vol0',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 1-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '1'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol1',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 1-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '1'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb1',
     'instance_id': 'inst0',
     'status': 'attached'}),
     'id': 'vol2',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb2',
     'instance_id': 'inst0',
     'status': 'attached'}),
     'id': 'vol3',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol4',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 3-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '3'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol5',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 3-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '3'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol6',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 3-3',
              'creator': 'testy (Testy Tester)',
              'index': '3',
              'logical': '/example/example.com/volume',
              'replica': '3'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol7',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 3-4',
              'creator': 'testy (Testy Tester)',
              'index': '4',
              'logical': '/example/example.com/volume',
              'replica': '3'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb3',
     'instance_id': 'inst0',
     'status': 'attached'}),
     'id': 'vol8',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-3',
              'creator': 'testy (Testy Tester)',
              'index': '3',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb4',
     'instance_id': 'inst0',
     'status': 'attached'}),
     'id': 'vol9',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-4',
              'creator': 'testy (Testy Tester)',
              'index': '4',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})


If you get a volume path wrong in a storage-server definition, the
server won't be created:

::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 4
           replicas = 3, 2

       /storage.example.com : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/volume 2'
       /storage2.example.com : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/database 2'

.. -> tree

    >>> zk.import_tree(tree)
    >>> server(['zookeeper.example.com:2181',
    ...        '/example/example.com/storage2.example.com'])
    Traceback (most recent call last):
    ...
    NoNodeException: /example/example.com/database

    >>> for instance in conn.instances:
    ...     print instance
    Instance({'attrs': {'groupSet': [Resource({'id': 'gr1',
     'name': 'default',
     'tags': {},
     'vpc_id': 'vpc1'})],
               'userData': "#!/bin/sh\necho 'example,storage' > /etc/zim/role\nhostname storage.example.com\n/usr/bin/yum -y install awsrecipes\n/opt/awsrecipes/bin/setup_volumes /example/example.com/storage.example.com\n"},
     'id': 'inst0',
     'tags': {'Name': 'storage.example.com',
              'creator': 'testy (Testy Tester)'}})

Setting up the volumns on the storage servers
=============================================

After the storage server is created, and the volumns are attached, we
need to set up the software raid, the logical volumes, and the mounts.

The setup_volumes scripts does this. It's run on the storage server
when it's created, as part of the user-data script (as shown in the
previous section). It can also be run later when additional ebs
volumes are added.

The script has to be careful to preserve any data that might already
be in the volume.  It does this by:

- Scanning the software raid and adding new raid volumes when it sees
  unconfigured ebs volumes and

- Scanning LVM data and adding new raid volumes to logical volumes, as
  necessary.


    >>> setup_volumes = pkg_resources.load_entry_point(
    ...     dist, 'console_scripts', 'setup-volumes')

Let's start with new ebs volumes.  For testing, we have a simulated
machine that we can initialize to similate a system and ebs volumes.
We'll start with 4 clean new ebs volumes as defined in the tree model:

    >>> volumes.init(['sdb1', 'sdb2', 'sdb3', 'sdb4'])

Now, we'll call setup_volumes:

    >>> setup_volumes(['zookeeper.example.com:2181',
    ...                '/example/example.com/storage.example.com'])
    /sbin/mdadm --examine --scan >>/etc/mdadm.conf
    /sbin/mdadm -A --scan
    /usr/sbin/vgscan
    /usr/sbin/pvscan
    /sbin/mdadm --create --metadata 1.2 -l10 -n4 /dev/md0 sdb1 sdb2 sdb3 sdb4
    /usr/sbin/vgcreate vg_sdb /dev/md0
    /usr/sbin/lvcreate -l +100%FREE -n data vg_sdb
    /sbin/mkfs -t ext3 /dev/vg_sdb/data
    /bin/mkdir -p /home/databases/example/example.com/volume
    /bin/mount -t ext3 /dev/vg_sdb/data /home/databases/example/example.com/volume

And if we look at the state of the setup machine, it's what we expect:

    >>> volumes.status()
    vg_sdb /home/databases/example/example.com/volume
        md0 [u'sdb1', u'sdb2', u'sdb3', u'sdb4']

If we terminate the machine and start a new machine with the same
volumes:

    >>> volumes.terminate()
    >>> setup_volumes(['zookeeper.example.com:2181',
    ...                '/example/example.com/storage.example.com'])
    /sbin/mdadm --examine --scan >>/etc/mdadm.conf
    /sbin/mdadm -A --scan
    /usr/sbin/vgscan
    /sbin/vgchange -a y vg_sdb
    /usr/sbin/pvscan
    /bin/mkdir -p /home/databases/example/example.com/volume
    /bin/mount -t ext3 /dev/vg_sdb/data /home/databases/example/example.com/volume

    >>> volumes.status()
    vg_sdb /home/databases/example/example.com/volume
        md0 [u'sdb1', u'sdb2', u'sdb3', u'sdb4']

Let's try that again, but this time, we'll add some more disks::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 8
           replicas = 3, 2
       /volume2 : lebs
           size = 1
           n = 4
           replicas = 3, 2

       /storage.example.com : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/volume 2'
         sdc = '/example/example.com/volume2 2'

.. -> tree

    >>> zk.import_tree(tree, trim=True)

Here, we've added disks to the first volume, and added a second
volume.

    >>> volumes.terminate()
    >>> volumes.sds.extend(
    ...     ['sdb5', 'sdb6', 'sdb7', 'sdb8', 'sdc1', 'sdc2', 'sdc3', 'sdc4'])

    >>> setup_volumes(['zookeeper.example.com:2181',
    ...                '/example/example.com/storage.example.com'])
    /sbin/mdadm --examine --scan >>/etc/mdadm.conf
    /sbin/mdadm -A --scan
    /usr/sbin/vgscan
    /sbin/vgchange -a y vg_sdb
    /usr/sbin/pvscan
    /sbin/mdadm --create --metadata 1.2 -l10 -n4 /dev/md1 sdb5 sdb6 sdb7 sdb8
    /usr/sbin/pvcreate /dev/md1
    /usr/sbin/vgextend vg_sdb /dev/md1
    /usr/sbin/lvextend -l +100%FREE /dev/vg_sdb/data
    /sbin/resize2fs /dev/vg_sdb/data
    /bin/mkdir -p /home/databases/example/example.com/volume
    /bin/mount -t ext3 /dev/vg_sdb/data /home/databases/example/example.com/volume
    /sbin/mdadm --create --metadata 1.2 -l10 -n4 /dev/md2 sdc1 sdc2 sdc3 sdc4
    /usr/sbin/vgcreate vg_sdc /dev/md2
    /usr/sbin/lvcreate -l +100%FREE -n data vg_sdc
    /sbin/mkfs -t ext3 /dev/vg_sdc/data
    /bin/mkdir -p /home/databases/example/example.com/volume2
    /bin/mount -t ext3 /dev/vg_sdc/data /home/databases/example/example.com/volume2

    >>> volumes.status()
    vg_sdb /home/databases/example/example.com/volume
        md0 [u'sdb1', u'sdb2', u'sdb3', u'sdb4']
        md1 [u'sdb5', u'sdb6', u'sdb7', u'sdb8']
    vg_sdc /home/databases/example/example.com/volume2
        md2 [u'sdc1', u'sdc2', u'sdc3', u'sdc4']
