Utility scripts
===============

lebs
----

Deploy or update a logical volume defined at a given ZooKeeper path.

Usage: ``lebs ZOO PATH``

where:

ZOO
  A ZooKeeper connection string.

PATH
  A path in ZooKeeper where a logical ebs volume is defined.  It must
  have a type property with the value: ``lebs``

Notes:

- The ZooKeeper tree must have a ``/hosts`` node containing a ``cluster``
  property naming the VPC/cluster that will be used.

Let's set up a sample tree::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 2
           replicas = 1, 2

  /hosts
    cluster = 'test_cluster'

.. -> tree

    >>> import zc.zk
    >>> zk = zc.zk.ZK('zookeeper.example.com:2181')
    >>> zk.import_tree(tree)

    >>> import pkg_resources
    >>> dist = pkg_resources.working_set.find(
    ...     pkg_resources.Requirement.parse('zc.awsrecipes'))
    >>> lebs = pkg_resources.load_entry_point(dist, 'console_scripts', 'lebs')
    >>> lebs(['zookeeper.example.com:2181', '/example/example.com/volume'])
    created /example/example.com/volume 1-1
    created /example/example.com/volume 1-2
    created /example/example.com/volume 2-1
    created /example/example.com/volume 2-2

    >>> import boto.ec2
    >>> conn = boto.ec2.connect_to_region('test_region')
    >>> for volume in conn.volumes:
    ...     print volume
    Volume({'id': 'vol0',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 1-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '1'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol1',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 1-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '1'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol2',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol3',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})

If we change the size after there are existing volumes, we'll get an error::

  /example
    /example.com
       /volume : lebs
           size = 2
           n = 2
           replicas = 1, 2

.. -> tree

    >>> zk.import_tree(tree)
    >>> lebs(['zookeeper.example.com:2181', '/example/example.com/volume'])
    Traceback (most recent call last):
    ...
    AssertionError: Existing volumne, /example/example.com/volume 1-1, has size 1

If we increase the number of volumes, or add a replica, additional
volumes are added as necessary::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 4
           replicas = 3, 2

.. -> tree

    >>> zk.import_tree(tree)
    >>> lebs(['zookeeper.example.com:2181', '/example/example.com/volume'])
    ... # doctest: +NORMALIZE_WHITESPACE
    created /example/example.com/volume 3-1
    created /example/example.com/volume 3-2
    created /example/example.com/volume 3-3
    created /example/example.com/volume 3-4
    exists /example/example.com/volume 2-1
    exists /example/example.com/volume 2-2
    created /example/example.com/volume 2-3
    created /example/example.com/volume 2-4
    Unused: ['/example/example.com/volume 1-1',
             '/example/example.com/volume 1-2']

Note that we stopped using replica 1.  The corresponding unused
volumes were noted, but not removed.  We're too cowardly to remove
disk volumes.

Once the volumes have been created, we're ready to set up storage
servers.  We'll start with one::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 4
           replicas = 3, 2

       /storage : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/volume 2'

.. -> tree

    >>> zk.import_tree(tree)

To add a server, we'll use the storage-server command:

    >>> server = pkg_resources.load_entry_point(
    ...     dist, 'console_scripts', 'storage-server')


    >>> server(['zookeeper.example.com:2181',
    ...        '/example/example.com/storage'])


At this point, we have a storage server:

    >>> for instance in conn.instances:
    ...     print instance
    Instance({'attrs': {'groupSet': [Resource({'id': 'gr1',
     'name': 'two-VPCSecurityGroup-QHALEVN2EWCD',
     'tags': {},
     'vpc_id': 'vpc1'})],
               'userData': "#!/bin/sh\necho 'example,storage' > /etc/zim/role\nhostname storage.test_cluster.aws.zope.net\n/usr/bin/yum -y install awsrecipes\n/opt/awsrecipes/bin/setup_volumes /example/example.com/storage\n"},
     'id': 'inst0',
     'tags': {'Name': 'storage.test_cluster.aws.zope.net',
              'creator': 'testy (Testy Tester)'}})

And the relevent volumes are attached:

    >>> for volume in conn.volumes:
    ...     print volume
    Volume({'id': 'vol0',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 1-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '1'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol1',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 1-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '1'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb1',
     'instance_id': 'inst0',
     'status': 'attached'}),
     'id': 'vol2',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb2',
     'instance_id': 'inst0',
     'status': 'attached'}),
     'id': 'vol3',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol4',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 3-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example.com/volume',
              'replica': '3'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol5',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 3-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example.com/volume',
              'replica': '3'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol6',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 3-3',
              'creator': 'testy (Testy Tester)',
              'index': '3',
              'logical': '/example/example.com/volume',
              'replica': '3'},
     'zone': 'us-up-1z'})
    Volume({'id': 'vol7',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 3-4',
              'creator': 'testy (Testy Tester)',
              'index': '4',
              'logical': '/example/example.com/volume',
              'replica': '3'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb3',
     'instance_id': 'inst0',
     'status': 'attached'}),
     'id': 'vol8',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-3',
              'creator': 'testy (Testy Tester)',
              'index': '3',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb4',
     'instance_id': 'inst0',
     'status': 'attached'}),
     'id': 'vol9',
     'size': 1,
     'tags': {'Name': '/example/example.com/volume 2-4',
              'creator': 'testy (Testy Tester)',
              'index': '4',
              'logical': '/example/example.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})


If you get a volume path wrong in a storage-server definition, the
server won't be created:

::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 4
           replicas = 3, 2

       /storage : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/volume 2'
       /storage2.example.com : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/database 2'

.. -> tree

    >>> zk.import_tree(tree)
    >>> server(['zookeeper.example.com:2181',
    ...        '/example/example.com/storage2.example.com'])
    Traceback (most recent call last):
    ...
    NoNodeException: /example/example.com/database

    >>> for instance in conn.instances:
    ...     print instance
    Instance({'attrs': {'groupSet': [Resource({'id': 'gr1',
     'name': 'two-VPCSecurityGroup-QHALEVN2EWCD',
     'tags': {},
     'vpc_id': 'vpc1'})],
               'userData': "#!/bin/sh\necho 'example,storage' > /etc/zim/role\nhostname storage.test_cluster.aws.zope.net\n/usr/bin/yum -y install awsrecipes\n/opt/awsrecipes/bin/setup_volumes /example/example.com/storage\n"},
     'id': 'inst0',
     'tags': {'Name': 'storage.test_cluster.aws.zope.net',
              'creator': 'testy (Testy Tester)'}})

Let's try multiple volumes:

  /example
    /example2.com
       /volume : lebs
           size = 1
           n = 4
           replicas = 2
       /v2 : lebs
           size = 1
           n = 4
           replicas = 2

       /storage.example2.com : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example2.com/volume 2'
         sdc = '/example/example2.com/v2 2'

.. -> tree

    >>> zk.import_tree(tree)
    extra path not trimmed: /example/example.com

    >>> server(['zookeeper.example.com:2181',
    ...        '/example/example2.com/storage.example2.com'])
    Traceback (most recent call last):
    ...
    AssertionError: ('Missing volumes', u'/example/example2.com/volume', 4, [])

Oops, we forgot to create the new volumes:

    >>> lebs(['zookeeper.example.com:2181', '/example/example2.com/volume'])
    created /example/example2.com/volume 2-1
    created /example/example2.com/volume 2-2
    created /example/example2.com/volume 2-3
    created /example/example2.com/volume 2-4
    >>> lebs(['zookeeper.example.com:2181', '/example/example2.com/v2'])
    created /example/example2.com/v2 2-1
    created /example/example2.com/v2 2-2
    created /example/example2.com/v2 2-3
    created /example/example2.com/v2 2-4

    >>> server(['zookeeper.example.com:2181',
    ...        '/example/example2.com/storage.example2.com'])


Double check the new attachments.

    >>> for volume in conn.volumes:
    ...     print volume
    ... # doctest: +ELLIPSIS
    Volume({'id': 'vol0',
    ...
    Volume({'attach_data': AttachmentSet({'device': u'sdb1',
     'instance_id': 'inst1',
     'status': 'attached'}),
     'id': 'vol10',
     'size': 1,
     'tags': {'Name': '/example/example2.com/volume 2-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example2.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb2',
     'instance_id': 'inst1',
     'status': 'attached'}),
     'id': 'vol11',
     'size': 1,
     'tags': {'Name': '/example/example2.com/volume 2-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example2.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb3',
     'instance_id': 'inst1',
     'status': 'attached'}),
     'id': 'vol12',
     'size': 1,
     'tags': {'Name': '/example/example2.com/volume 2-3',
              'creator': 'testy (Testy Tester)',
              'index': '3',
              'logical': '/example/example2.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdb4',
     'instance_id': 'inst1',
     'status': 'attached'}),
     'id': 'vol13',
     'size': 1,
     'tags': {'Name': '/example/example2.com/volume 2-4',
              'creator': 'testy (Testy Tester)',
              'index': '4',
              'logical': '/example/example2.com/volume',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdc1',
     'instance_id': 'inst1',
     'status': 'attached'}),
     'id': 'vol14',
     'size': 1,
     'tags': {'Name': '/example/example2.com/v2 2-1',
              'creator': 'testy (Testy Tester)',
              'index': '1',
              'logical': '/example/example2.com/v2',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdc2',
     'instance_id': 'inst1',
     'status': 'attached'}),
     'id': 'vol15',
     'size': 1,
     'tags': {'Name': '/example/example2.com/v2 2-2',
              'creator': 'testy (Testy Tester)',
              'index': '2',
              'logical': '/example/example2.com/v2',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdc3',
     'instance_id': 'inst1',
     'status': 'attached'}),
     'id': 'vol16',
     'size': 1,
     'tags': {'Name': '/example/example2.com/v2 2-3',
              'creator': 'testy (Testy Tester)',
              'index': '3',
              'logical': '/example/example2.com/v2',
              'replica': '2'},
     'zone': 'us-up-1z'})
    Volume({'attach_data': AttachmentSet({'device': u'sdc4',
     'instance_id': 'inst1',
     'status': 'attached'}),
     'id': 'vol17',
     'size': 1,
     'tags': {'Name': '/example/example2.com/v2 2-4',
     ...

Setting up the volumns on the storage servers
=============================================

After the storage server is created, and the volumns are attached, we
need to set up the software raid, the logical volumes, and the mounts.

The setup_volumes scripts does this. It's run on the storage server
when it's created, as part of the user-data script (as shown in the
previous section). It can also be run later when additional ebs
volumes are added.

The script has to be careful to preserve any data that might already
be in the volume.  It does this by:

- Scanning the software raid and adding new raid volumes when it sees
  unconfigured ebs volumes and

- Scanning LVM data and adding new raid volumes to logical volumes, as
  necessary.


    >>> setup_volumes = pkg_resources.load_entry_point(
    ...     dist, 'console_scripts', 'setup-volumes')

Let's start with new ebs volumes.  For testing, we have a simulated
machine that we can initialize to similate a system and ebs volumes.
We'll start with 4 clean new ebs volumes as defined in the tree model:

    >>> volumes.init(['sdb1', 'sdb2', 'sdb3', 'sdb4'])

Now, we'll call setup_volumes:

    >>> setup_volumes(['zookeeper.example.com:2181',
    ...                '/example/example.com/storage'])
    ... # doctest: +NORMALIZE_WHITESPACE
    /sbin/mdadm --examine --scan >>/etc/mdadm.conf
    /sbin/mdadm -A --scan
    /usr/sbin/vgscan
    /usr/sbin/pvscan
    /sbin/mdadm --create --metadata 1.2 -l10 -n4
       /dev/md0 /dev/sdb1 /dev/sdb2 /dev/sdb3 /dev/sdb4
    /usr/sbin/vgcreate vg_sdb /dev/md0
    /usr/sbin/lvcreate -l +100%FREE -n data vg_sdb
    /sbin/mkfs -t ext3 /dev/vg_sdb/data
    /bin/mkdir -p /home/databases/example/example.com
    /bin/mount -t ext3 /dev/vg_sdb/data /home/databases/example/example.com

And if we look at the state of the setup machine, it's what we expect:

    >>> volumes.status()
    vg_sdb /home/databases/example/example.com
        md0 [u'sdb1', u'sdb2', u'sdb3', u'sdb4']

Note that the mount point defaults to the volume path without the past
path segment.

If we terminate the machine and start a new machine with the same
volumes:

    >>> volumes.terminate()
    >>> setup_volumes(['zookeeper.example.com:2181',
    ...                '/example/example.com/storage'])
    /sbin/mdadm --examine --scan >>/etc/mdadm.conf
    /sbin/mdadm -A --scan
    /usr/sbin/vgscan
    /sbin/vgchange -a y vg_sdb
    /usr/sbin/pvscan
    /bin/mkdir -p /home/databases/example/example.com
    /bin/mount -t ext3 /dev/vg_sdb/data /home/databases/example/example.com

    >>> volumes.status()
    vg_sdb /home/databases/example/example.com
        md0 [u'sdb1', u'sdb2', u'sdb3', u'sdb4']

Let's try that again, but this time, we'll add some more disks::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 8
           replicas = 3, 2
       /volume2 : lebs
           size = 1
           n = 4
           replicas = 3, 2

       /storage : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/volume 2'
         sdc = '/example/example.com/volume2 2'

.. -> tree

    >>> zk.import_tree(tree, trim=True)

Here, we've added disks to the first volume, and added a second
volume.

    >>> volumes.terminate()
    >>> volumes.sds.extend(
    ...     ['sdb5', 'sdb6', 'sdb7', 'sdb8', 'sdc1', 'sdc2', 'sdc3', 'sdc4'])

    >>> setup_volumes(['zookeeper.example.com:2181',
    ...                '/example/example.com/storage'])
    Traceback (most recent call last):
    ...
    ValueError: ('Duplicate mount points', u'/example/example.com')

Oops. Remember that by default, the volume path *without* the last
path segment is the default mount point.  In this case, this leads to
duplication.  We can provide an explicit path::

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 8
           replicas = 3, 2
       /volume2 : lebs
           size = 1
           n = 4
           replicas = 3, 2
           path = '/example/example.com/volume2'

       /storage : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/volume 2'
         sdc = '/example/example.com/volume2 2'

.. -> tree

    >>> zk.import_tree(tree, trim=True)

But we'll still get an error::

    >>> setup_volumes(['zookeeper.example.com:2181',
    ...                '/example/example.com/storage'])
    ... # doctest: +NORMALIZE_WHITESPACE
    Traceback (most recent call last):
    ...
    ValueError: ('One mount point is a prefix of another',
                 u'/example/example.com/', u'/example/example.com/volume2')

Here we failed because our mountpoints overlap. Not good.  Let's try
again:

  /example
    /example.com
       /volume : lebs
           size = 1
           n = 8
           replicas = 3, 2
       /volume2 : lebs
           size = 1
           n = 4
           replicas = 3, 2
           path = '/example/other'

       /storage : storage-server
         instance-type = 'm1.large'
         sdb = '/example/example.com/volume 2'
         sdc = '/example/example.com/volume2 2'


.. -> tree

    >>> zk.import_tree(tree, trim=True)

    >>> setup_volumes(['zookeeper.example.com:2181',
    ...                '/example/example.com/storage'])
    ... # doctest: +NORMALIZE_WHITESPACE
    /sbin/mdadm --examine --scan >>/etc/mdadm.conf
    /sbin/mdadm -A --scan
    /usr/sbin/vgscan
    /sbin/vgchange -a y vg_sdb
    /usr/sbin/pvscan
    /sbin/mdadm --create --metadata 1.2 -l10 -n4
       /dev/md1 /dev/sdb5 /dev/sdb6 /dev/sdb7 /dev/sdb8
    /usr/sbin/pvcreate /dev/md1
    /usr/sbin/vgextend vg_sdb /dev/md1
    /usr/sbin/lvextend -l +100%FREE /dev/vg_sdb/data
    /sbin/resize2fs /dev/vg_sdb/data
    /bin/mkdir -p /home/databases/example/example.com
    /bin/mount -t ext3 /dev/vg_sdb/data /home/databases/example/example.com
    /sbin/mdadm --create --metadata 1.2 -l10 -n4
         /dev/md2 /dev/sdc1 /dev/sdc2 /dev/sdc3 /dev/sdc4
    /usr/sbin/vgcreate vg_sdc /dev/md2
    /usr/sbin/lvcreate -l +100%FREE -n data vg_sdc
    /sbin/mkfs -t ext3 /dev/vg_sdc/data
    /bin/mkdir -p /home/databases/example/other
    /bin/mount -t ext3 /dev/vg_sdc/data /home/databases/example/other

    >>> volumes.status()
    vg_sdb /home/databases/example/example.com
        md0 [u'sdb1', u'sdb2', u'sdb3', u'sdb4']
        md1 [u'sdb5', u'sdb6', u'sdb7', u'sdb8']
    vg_sdc /home/databases/example/other
        md2 [u'sdc1', u'sdc2', u'sdc3', u'sdc4']
